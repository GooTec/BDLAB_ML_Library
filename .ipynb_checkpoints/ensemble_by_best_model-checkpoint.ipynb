{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from lib import dataProcess as dp\n",
    "from lib import deepLearning as dl\n",
    "\n",
    "import sys, getopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_options(argv):\n",
    "    inputfiles = ''\n",
    "    outputfile = ''\n",
    "    inputmodels = ''\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hi:m:o:\",[\"ifiles=\", \"imodels=\",\"ofile=\"])\n",
    "    except getopt.GetoptError:\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print('help')\n",
    "            sys.exit()\n",
    "        elif opt in (\"-i\", \"--ifiles\"):\n",
    "            inputfiles = arg\n",
    "        elif opt in (\"-o\", \"--ofile\"):\n",
    "            outputfile = arg\n",
    "        elif opt in (\"-m\", \"--imodels\"):\n",
    "            inputmodels = arg\n",
    "        \n",
    "    print('Input files are ', inputfiles)\n",
    "    print('Input models are ', inputmodels)\n",
    "    return inputfiles, inputmodels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inputfiles, inputmodels = get_options(sys.argv[1:])\n",
    "    inputfiles_f = open(inputfiles, 'r')\n",
    "    inputmodels_f = open(inputmodels, 'r')\n",
    "    inputfiles = inputfiles_f.readlines()\n",
    "    inputmodels = inputmodels_f.readlines()\n",
    "    inputfiles_f.close()\n",
    "    inputmodels_f.close()\n",
    "    ###Setting filelist and modellist\n",
    "    try :\n",
    "        for i in range(len(inputfiles)):\n",
    "            inputfile = inputfiles[i]\n",
    "            modelfile = modelfiles[i]\n",
    "            if inputfile[-1] == '\\n' or inputfile[-1] == ' ':\n",
    "                inputfiles[i] = inputfile[:-1]\n",
    "            if modelfile[-1] == '\\n' or modelfile[-1] == ' ':\n",
    "                modelfiles[i] = modelfile[:-1]\n",
    "    \n",
    "    ###Setting hypothesis lists###\n",
    "    train_h_list = None\n",
    "    test_h_list = None\n",
    "    val_h_list = None\n",
    "    \n",
    "    ###read files and models to get hypothesis\n",
    "    nodes = [200,100,50]\n",
    "    for i in range(len(inputfiles)):\n",
    "        raw_data = pd.read_csv(inputfiles[i])\n",
    "        fivefold = dp.fivefold(raw_data, 'index')\n",
    "        xdata_five, ydata_five = dp.divide_xy_train(fivefold, 'result', True, 1, -3)\n",
    "        train_x, test_x = dp.train_test(xdata_five, 0)\n",
    "        train_y, test_y = dp.train_test(ydata_five, 0)\n",
    "        train_y = dp.one_hot_encoder(train_y)\n",
    "        test_y = dp.one_hot_encoder(test_y)\n",
    "        val_x, test_x = dp.test_validation(test_x)\n",
    "        val_y, test_y = dp.test_validation(test_y)\n",
    "        batch_size = 100\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        X, Y, weights, bias, hidden_layers, logits, hypothesis, cost, train, predicted, correct_prediction, accuracy, keep_prob = dl.set_model(\n",
    "                xdata, ydata, nodes, learning_rate)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, modelfiles[i])\n",
    "            train_h, train_a = sess.run([hypothesis, accuracy ], feed_dict={X: train_x, Y: train_y, keep_prob: 1.0})\n",
    "            val_h, val_a = sess.run([hypothesis, accuracy ], feed_dict={X: val_x, Y: val_y, keep_prob: 1.0})\n",
    "            test_h, test_a = sess.run([hypothesis, accuracy ], feed_dict={X: test_x, Y: test_y, keep_prob: 1.0})\n",
    "            \n",
    "            if train_h_list == None :\n",
    "                train_h_list = train_h[:, 1]\n",
    "            else : \n",
    "                train_h_list = np.concatenate((train_h_list, train_h[:,1]) , axis = 1)\n",
    "            \n",
    "            if val_h_list == None :\n",
    "                val_h_list = val_h[:, 1]\n",
    "            else : \n",
    "                val_h_list = np.concatenate((val_h_list, val_h[:,1]) , axis = 1)\n",
    "                \n",
    "            if test_h_list == None :\n",
    "                test_h_list = test_h[:, 1]\n",
    "            else : \n",
    "                test_h_list = np.concatenate((test_h_list, test_h[:,1]) , axis = 1)\n",
    "                \n",
    "    batch_size = 100\n",
    "    learning_rate = 0.001\n",
    "    nodes = [20,20,20]\n",
    "    save_path_dir = './'\n",
    "    X, Y, weights, bias, hidden_layers, logits, hypothesis, cost, train, predicted, correct_prediction, accuracy, keep_prob = dl.set_model( train_h_list, ydata, nodes, learning_rate)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize TensorFlow variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        stop_switch = True\n",
    "        step = 0\n",
    "\n",
    "        while stop_switch:\n",
    "            total_num = int(len(train_h_list) / batch_size)\n",
    "            for k in range(total_num):\n",
    "                batch_x = train_h_list[k * batch_size:(k + 1) * batch_size]\n",
    "                batch_y = train_h_list[k * batch_size:(k + 1) * batch_size]\n",
    "                sess.run(train, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.5})\n",
    "\n",
    "\n",
    "            train_h, train_c, train_p, train_a = sess.run( [hypothesis, cost, predicted, accuracy], feed_dict={X: train_h_list, Y: train_y, keep_prob: 1})\n",
    "            val_h, val_c, val_p, val_a = sess.run([hypothesis, cost, predicted, accuracy], feed_dict={X: val_h_list, Y: val_y, keep_prob: 1})\n",
    "            if step % 20 == 0 :\n",
    "                print(\"train acc : \", train_a, \"validation acc : \", val_a, \"train_cost\", train_c)\n",
    "            step += 1\n",
    "\n",
    "            if best_cost > val_c :\n",
    "                best_train_acc = train_a\n",
    "                best_val_acc = val_a\n",
    "                best_cost = val_c\n",
    "                count = 0\n",
    "                save_path = saver.save(sess, save_path_dir + 'model'+str(model_num)+'.ckpt')\n",
    "\n",
    "            elif count > 10 :\n",
    "                stop_switch = False\n",
    "                print(\"Learning Finished!! \\n\")\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "        print(\"Training Accuracy : \", best_train_acc,  \"Validation Accuracy : \", best_val_acc)\n",
    "\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "        test_h, test_p, test_a = sess.run([hypothesis, predicted, accuracy],\n",
    "                                          feed_dict={X: test_h_list, Y: test_y, keep_prob: 1.0})\n",
    "        print(\"\\nTest Accuracy: \", test_a)\n",
    "        best_test_acc = test_a\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
